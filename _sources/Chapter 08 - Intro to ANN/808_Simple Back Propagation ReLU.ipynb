{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "441ce87d",
   "metadata": {},
   "source": [
    "Sure, I'll provide a full example of using gradient descent to train a simple artificial neural network (ANN) with one ReLU-activated neuron. In this example, we'll implement a single-variable regression task. We'll create a network with one input neuron, one hidden layer with one ReLU-activated neuron, and one output neuron.\n",
    "\n",
    "**Step 1: Initialization**\n",
    "\n",
    "Let's initialize the network's parameters:\n",
    "\n",
    "- Weight of the hidden neuron (\\(w\\)): Initialize randomly, e.g., \\(w = 0.5\\).\n",
    "- Bias of the hidden neuron (\\(b\\)): Initialize randomly, e.g., \\(b = 0.2\\).\n",
    "- Weight of the output neuron (\\(v\\)): Initialize randomly, e.g., \\(v = 0.3\\).\n",
    "- Bias of the output neuron (\\(c\\)): Initialize randomly, e.g., \\(c = 0.1\\).\n",
    "\n",
    "Hyperparameters:\n",
    "\n",
    "- Learning rate (\\(\\alpha\\)): Choose a suitable learning rate, e.g., \\(\\alpha = 0.01\\).\n",
    "\n",
    "**Step 2: Forward Pass**\n",
    "\n",
    "For each training example, perform the forward pass through the network:\n",
    "\n",
    "- Input (\\(x\\)): The input data.\n",
    "- Hidden layer output (\\(h\\)): Apply the ReLU activation function.\n",
    "\n",
    "   \\[h = \\text{ReLU}(w \\cdot x + b)\\]\n",
    "\n",
    "- Output (\\(y\\)): Compute the network's output.\n",
    "\n",
    "   \\[y = v \\cdot h + c\\]\n",
    "\n",
    "**Step 3: Compute the Loss**\n",
    "\n",
    "Calculate the loss using a loss function, such as mean squared error (MSE):\n",
    "\n",
    "- Target output (\\(y_{\\text{target}}\\)): The desired output for the given input.\n",
    "\n",
    "   \\[J(\\theta) = \\frac{1}{2}(y - y_{\\text{target}})^2\\]\n",
    "\n",
    "**Step 4: Backpropagation**\n",
    "\n",
    "Calculate the gradients of the loss with respect to the parameters (\\(w\\), \\(b\\), \\(v\\), \\(c\\)) using backpropagation:\n",
    "\n",
    "- Compute the gradient of the loss with respect to the output (\\(\\frac{\\partial J}{\\partial y}\\)) and use the chain rule to compute the gradients for the other parameters.\n",
    "\n",
    "   \\(\\frac{\\partial J}{\\partial v} = \\frac{\\partial J}{\\partial y} \\cdot h\\)\n",
    "\n",
    "   \\(\\frac{\\partial J}{\\partial c} = \\frac{\\partial J}{\\partial y}\\)\n",
    "\n",
    "   \\(\\frac{\\partial J}{\\partial h} = \\frac{\\partial J}{\\partial y} \\cdot v\\)\n",
    "\n",
    "   \\(\\frac{\\partial J}{\\partial w} = \\frac{\\partial J}{\\partial h} \\cdot \\frac{\\partial h}{\\partial(w \\cdot x + b)} \\cdot x\\)\n",
    "\n",
    "   \\(\\frac{\\partial J}{\\partial b} = \\frac{\\partial J}{\\partial h} \\cdot \\frac{\\partial h}{\\partial(w \\cdot x + b)}\\)\n",
    "\n",
    "- Update the parameters using gradient descent:\n",
    "\n",
    "   - Update \\(v\\) and \\(c\\) using the computed gradients and the learning rate (\\(\\alpha\\)).\n",
    "\n",
    "   - Update \\(w\\) and \\(b\\) using the computed gradients and the learning rate (\\(\\alpha\\)).\n",
    "\n",
    "Repeat steps 2-4 for a specified number of iterations or until the loss converges to a minimum. This process will train the network to make accurate predictions for the given input data. The choice of learning rate and the number of iterations can significantly affect the training process, and tuning these hyperparameters is an important part of training neural networks effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a262b931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Cost = 3.6180, Predicted Output = 0.31\n",
      "\n",
      "Trained Parameters:\n",
      "w 0.497: b = 0.1970, v = 0.29, c = 0.29\n",
      "Iteration 1: Cost = 3.6630, Predicted Output = 0.29\n",
      "\n",
      "Trained Parameters:\n",
      "w 0.49407: b = 0.1941, v = 0.29, c = 0.29\n",
      "Iteration 2: Cost = 3.7078, Predicted Output = 0.28\n",
      "\n",
      "Trained Parameters:\n",
      "w 0.4912094: b = 0.1912, v = 0.28, c = 0.28\n",
      "Iteration 3: Cost = 3.7524, Predicted Output = 0.26\n",
      "\n",
      "Trained Parameters:\n",
      "w 0.488417614: b = 0.1884, v = 0.27, c = 0.27\n",
      "Iteration 4: Cost = 3.7968, Predicted Output = 0.24\n",
      "\n",
      "Trained Parameters:\n",
      "w 0.48569406988: b = 0.1857, v = 0.27, c = 0.27\n",
      "Iteration 5: Cost = 3.8411, Predicted Output = 0.23\n",
      "\n",
      "Trained Parameters:\n",
      "w 0.4830382092828: b = 0.1830, v = 0.26, c = 0.26\n",
      "Iteration 6: Cost = 3.8853, Predicted Output = 0.21\n",
      "\n",
      "Trained Parameters:\n",
      "w 0.480449487499576: b = 0.1804, v = 0.25, c = 0.25\n",
      "Iteration 7: Cost = 3.9293, Predicted Output = 0.20\n",
      "\n",
      "Trained Parameters:\n",
      "w 0.47792737335820856: b = 0.1779, v = 0.25, c = 0.25\n",
      "Iteration 8: Cost = 3.9732, Predicted Output = 0.18\n",
      "\n",
      "Trained Parameters:\n",
      "w 0.475471349114341: b = 0.1755, v = 0.24, c = 0.24\n",
      "Iteration 9: Cost = 4.0169, Predicted Output = 0.17\n",
      "\n",
      "Trained Parameters:\n",
      "w 0.47308091034514516: b = 0.1731, v = 0.23, c = 0.23\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Initialize parameters\n",
    "w = 0.5\n",
    "b = 0.2\n",
    "v = 0.3\n",
    "c = 0.1\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Training data\n",
    "x = 1  # Input feature\n",
    "y_target = 3# Target output\n",
    "\n",
    "# Number of training iterations\n",
    "num_iterations = 10\n",
    "\n",
    "# Gradient Descent\n",
    "for i in range(num_iterations):\n",
    "    # Forward pass\n",
    "    h = max(0, w * x + b)  # ReLU activation\n",
    "    y_pred = v * h + c\n",
    "    \n",
    "    # Compute the cost (MSE)\n",
    "    cost = 0.5 * (y_pred - y_target)**2\n",
    "    \n",
    "    # Backpropagation\n",
    "    # Compute gradients\n",
    "    dv = h\n",
    "    dc = 1\n",
    "    dh = v\n",
    "    \n",
    "    if w * x + b > 0:\n",
    "        dw = x * v\n",
    "        db = v\n",
    "    else:\n",
    "        dw = 0\n",
    "        db = 0\n",
    "    \n",
    "    # Update parameters using gradient descent\n",
    "    w =w- learning_rate * dw\n",
    "    b =b- learning_rate * db\n",
    "    v =v- learning_rate * dv\n",
    "    c =c- learning_rate * dc\n",
    "    \n",
    "    # Print progress\n",
    "    print(f\"Iteration {i}: Cost = {cost:.4f}, Predicted Output = {y_pred:.2f}\")\n",
    "    print(\"\\nTrained Parameters:\")\n",
    "    print(f\"w {w}: b = {b:.4f}, v = {v:.2f}, c = {v:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e036f7a6",
   "metadata": {},
   "source": [
    "Certainly! Let's walk through a complete example of training a neural network with a single ReLU (Rectified Linear Unit) neuron using gradient descent. In this example, we'll create a simple artificial neural network (ANN) with one input feature, one hidden layer containing one ReLU neuron, and one output neuron.\n",
    "\n",
    "### Neural Network Architecture:\n",
    "- **Input Layer**: One neuron (\\(x\\)).\n",
    "- **Hidden Layer**: One ReLU neuron (\\(h\\)) with weight (\\(w\\)) and bias (\\(b\\)).\n",
    "- **Output Layer**: One neuron (\\(y\\)) with weight (\\(v\\)) and bias (\\(c\\)).\n",
    "\n",
    "### Forward Pass:\n",
    "The output \\(y\\) of the neural network is calculated as follows:\n",
    "\n",
    "\\[h = \\text{ReLU}(w \\cdot x + b)\\]\n",
    "\\[y = v \\cdot h + c\\]\n",
    "\n",
    "### Cost Function:\n",
    "Let's use Mean Squared Error (MSE) as the cost function:\n",
    "\n",
    "\\[J(w, b, v, c) = \\frac{1}{2}(y - y_{\\text{target}})^2\\]\n",
    "\n",
    "### Gradient Descent Updates:\n",
    "The gradients of the cost function with respect to the parameters (\\(w\\), \\(b\\), \\(v\\), \\(c\\)) are computed using backpropagation, and the parameters are updated using the gradient descent algorithm:\n",
    "\n",
    "\\[w = w - \\alpha \\cdot \\frac{\\partial J}{\\partial w}\\]\n",
    "\\[b = b - \\alpha \\cdot \\frac{\\partial J}{\\partial b}\\]\n",
    "\\[v = v - \\alpha \\cdot \\frac{\\partial J}{\\partial v}\\]\n",
    "\\[c = c - \\alpha \\cdot \\frac{\\partial J}{\\partial c}\\]\n",
    "\n",
    "Here's how you can implement and train this simple ReLU neural network using gradient descent in Python:\n",
    "\n",
    "```python\n",
    "# Initialize parameters\n",
    "w = 0.5\n",
    "b = 0.2\n",
    "v = 0.3\n",
    "c = 0.1\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Training data\n",
    "x = 1.0  # Input feature\n",
    "y_target = 3.0  # Target output\n",
    "\n",
    "# Number of training iterations\n",
    "num_iterations = 100\n",
    "\n",
    "# Gradient Descent\n",
    "for i in range(num_iterations):\n",
    "    # Forward pass\n",
    "    h = max(0, w * x + b)  # ReLU activation\n",
    "    y_pred = v * h + c\n",
    "    \n",
    "    # Compute the cost (MSE)\n",
    "    cost = 0.5 * (y_pred - y_target)**2\n",
    "    \n",
    "    # Backpropagation\n",
    "    # Compute gradients\n",
    "    dv = h\n",
    "    dc = 1\n",
    "    dh = v\n",
    "    \n",
    "    if w * x + b > 0:\n",
    "        dw = x * v\n",
    "        db = v\n",
    "    else:\n",
    "        dw = 0\n",
    "        db = 0\n",
    "    \n",
    "    # Update parameters using gradient descent\n",
    "    w -= learning_rate * dw\n",
    "    b -= learning_rate * db\n",
    "    v -= learning_rate * dv\n",
    "    c -= learning_rate * dc\n",
    "    \n",
    "    # Print progress\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Iteration {i}: Cost = {cost:.4f}, Predicted Output = {y_pred:.2f}\")\n",
    "\n",
    "# Final trained parameters\n",
    "print(\"\\nTrained Parameters:\")\n",
    "print(f\"w = {w:.2f}\")\n",
    "print(f\"b = {b:.2f}\")\n",
    "print(f\"v = {v:.2f}\")\n",
    "print(f\"c = {c:.2f}\")\n",
    "```\n",
    "\n",
    "In this example, the neural network is trained for 100 iterations using gradient descent. You can adjust the number of iterations, learning rate, and the initial parameters to see how the network's performance changes during training. The ReLU activation function is applied to the hidden layer output during the forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5bbab4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
