{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "441ce87d",
   "metadata": {},
   "source": [
    "# One Node Back Propagation ReLU\n",
    "\n",
    "In this document we calculate the back-propagation aglorithm for an Artificial Neural Network with one input $x$, one hidden layer with a single ReLU node, and out linear output node.\n",
    "\n",
    "**Step 1: Initialization**\n",
    "\n",
    "Let's initialize the network's parameters:\n",
    "\n",
    "- Weight of the hidden neuron ($w$): Initialize randomly, e.g., $w = 0.5$.\n",
    "- Bias of the hidden neuron ($b$): Initialize randomly, e.g., $b = 0.2$.\n",
    "- Weight of the output neuron ($v$): Initialize randomly, e.g., $v = 0.3$.\n",
    "- Bias of the output neuron ($c$): Initialize randomly, e.g., $c = 0.1$.\n",
    "\n",
    "Hyperparameters:\n",
    "\n",
    "- Learning rate ($\\alpha$): Choose a suitable learning rate, e.g., $\\alpha = 0.01$.\n",
    "\n",
    "**Step 2: Forward Pass**\n",
    "\n",
    "For each training example, perform the forward pass through the network:\n",
    "\n",
    "- Input ($x$): The input data.\n",
    "- Hidden layer output ($h$): Apply the ReLU activation function.\n",
    "\n",
    "   $$h = \\text{ReLU}(w \\cdot x + b)$$\n",
    "\n",
    "- Output ($y$): Compute the network's output.\n",
    "\n",
    "   $$y = v \\cdot h + c$$\n",
    "\n",
    "**Step 3: Compute the Loss**\n",
    "\n",
    "Calculate the loss using a loss function, such as mean squared error (MSE):\n",
    "\n",
    "- Target output ($y_{\\text{target}}$): The desired output for the given input.\n",
    "\n",
    "   $$J(\\theta) = \\frac{1}{2}(y - y_{\\text{target}})^2$$\n",
    "\n",
    "**Step 4: Backpropagation**\n",
    "\n",
    "Calculate the gradients of the loss with respect to the parameters ($w$, $b$, $v$, $c$) using backpropagation:\n",
    "\n",
    "- Compute the gradient of the loss with respect to the output ($\\frac{\\partial J}{\\partial y}$) and use the chain rule to compute the gradients for the other parameters.\n",
    "\n",
    "   $\\frac{\\partial J}{\\partial v} = \\frac{\\partial J}{\\partial y} \\cdot h$\n",
    "\n",
    "   $\\frac{\\partial J}{\\partial c} = \\frac{\\partial J}{\\partial y}$\n",
    "\n",
    "   $\\frac{\\partial J}{\\partial h} = \\frac{\\partial J}{\\partial y} \\cdot v$\n",
    "\n",
    "   $\\frac{\\partial J}{\\partial w} = \\frac{\\partial J}{\\partial h} \\cdot \\frac{\\partial h}{\\partial(w \\cdot x + b)} \\cdot x$\n",
    "\n",
    "   $\\frac{\\partial J}{\\partial b} = \\frac{\\partial J}{\\partial h} \\cdot \\frac{\\partial h}{\\partial(w \\cdot x + b)}$\n",
    "\n",
    "- Update the parameters using gradient descent:\n",
    "\n",
    "   - Update $v$ and $c$ using the computed gradients and the learning rate ($\\alpha$).\n",
    "\n",
    "   - Update $w$ and $b$ using the computed gradients and the learning rate ($\\alpha$).\n",
    "\n",
    "Repeat steps 2-4 for a specified number of iterations or until the loss converges to a minimum. This process will train the network to make accurate predictions for the given input data. The choice of learning rate and the number of iterations can significantly affect the training process, and tuning these hyperparameters is an important part of training neural networks effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a262b931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Cost = 3.6180, Predicted Output = 0.31\n",
      "\n",
      "Trained Parameters:\n",
      "w 0.50807: b = 0.2081, v = 0.32, c = 0.32\n",
      "Iteration 1: Cost = 3.4974, Predicted Output = 0.36\n",
      "\n",
      "Trained Parameters:\n",
      "w 0.5165023300230795: b = 0.2165, v = 0.34, c = 0.34\n",
      "Iteration 2: Cost = 3.3776, Predicted Output = 0.40\n",
      "\n",
      "Trained Parameters:\n",
      "w 0.5252811993729141: b = 0.2253, v = 0.36, c = 0.36\n",
      "Iteration 3: Cost = 3.2585, Predicted Output = 0.45\n",
      "\n",
      "Trained Parameters:\n",
      "w 0.5343902995969806: b = 0.2344, v = 0.38, c = 0.38\n",
      "Iteration 4: Cost = 3.1402, Predicted Output = 0.49\n",
      "\n",
      "Trained Parameters:\n",
      "w 0.5438127352962644: b = 0.2438, v = 0.40, c = 0.40\n",
      "Iteration 5: Cost = 3.0228, Predicted Output = 0.54\n",
      "\n",
      "Trained Parameters:\n",
      "w 0.553530966017995: b = 0.2535, v = 0.41, c = 0.41\n",
      "Iteration 6: Cost = 2.9061, Predicted Output = 0.59\n",
      "\n",
      "Trained Parameters:\n",
      "w 0.5635267600341148: b = 0.2635, v = 0.43, c = 0.43\n",
      "Iteration 7: Cost = 2.7904, Predicted Output = 0.64\n",
      "\n",
      "Trained Parameters:\n",
      "w 0.5737811608423549: b = 0.2738, v = 0.45, c = 0.45\n",
      "Iteration 8: Cost = 2.6757, Predicted Output = 0.69\n",
      "\n",
      "Trained Parameters:\n",
      "w 0.58427446722317: b = 0.2843, v = 0.47, c = 0.47\n",
      "Iteration 9: Cost = 2.5620, Predicted Output = 0.74\n",
      "\n",
      "Trained Parameters:\n",
      "w 0.5949862276409392: b = 0.2950, v = 0.49, c = 0.49\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Initialize parameters\n",
    "w = 0.5\n",
    "b = 0.2\n",
    "v = 0.3\n",
    "c = 0.1\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Training data\n",
    "x = 1  # Input feature\n",
    "y_target = 3# Target output\n",
    "\n",
    "# Number of training iterations\n",
    "num_iterations = 10\n",
    "\n",
    "# Gradient Descent\n",
    "for i in range(num_iterations):\n",
    "    # Forward pass\n",
    "    h = max(0, w * x + b)  # ReLU activation\n",
    "    y_pred = v * h + c\n",
    "    \n",
    "    # Compute the cost (MSE)\n",
    "    cost = 0.5 * (y_pred - y_target)**2\n",
    "    dy = (y_pred - y_target)\n",
    "    \n",
    "    # Backpropagation\n",
    "    # Compute gradients\n",
    "    dv = dy*h\n",
    "    dc = dy*1\n",
    "    dh = dy*v\n",
    "    \n",
    "    if w * x + b > 0:\n",
    "        dw = x * dh\n",
    "        db = dh\n",
    "    else:\n",
    "        dw = 0\n",
    "        db = 0\n",
    "    \n",
    "    # Update parameters using gradient descent\n",
    "    w =w- learning_rate * dw\n",
    "    b =b- learning_rate * db\n",
    "    v =v- learning_rate * dv\n",
    "    c =c- learning_rate * dc\n",
    "    \n",
    "    # Print progress\n",
    "    print(f\"Iteration {i}: Cost = {cost:.4f}, Predicted Output = {y_pred:.2f}\")\n",
    "    print(\"\\nTrained Parameters:\")\n",
    "    print(f\"w {w}: b = {b:.4f}, v = {v:.2f}, c = {v:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ece23f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}