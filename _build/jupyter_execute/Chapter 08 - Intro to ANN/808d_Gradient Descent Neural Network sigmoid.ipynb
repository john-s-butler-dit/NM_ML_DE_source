{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f4ae5b7",
   "metadata": {},
   "source": [
    "Certainly! Let's demonstrate gradient descent in the context of training a simple neural network with a single input neuron, one hidden layer with one neuron, and a single output neuron. We'll use a basic example to help you understand the concept. \n",
    "\n",
    "Here's a simplified neural network architecture:\n",
    "\n",
    "1. Input layer: One neuron with input \\(x\\).\n",
    "2. Hidden layer: One neuron with weight \\(w\\) and bias \\(b\\), applying the sigmoid activation function.\n",
    "3. Output layer: One neuron with weight \\(v\\) and bias \\(c\\), applying the linear (identity) activation function.\n",
    "\n",
    "The network's output \\(y\\) can be expressed as:\n",
    "\n",
    "\\[y = v \\cdot \\text{sigmoid}(w \\cdot x + b) + c\\]\n",
    "\n",
    "Your goal is to use gradient descent to train the network by updating the weights and biases to minimize a cost function \\(J(\\theta)\\), where \\(\\theta\\) represents all the network parameters (\\(w\\), \\(b\\), \\(v\\), and \\(c\\)).\n",
    "\n",
    "**Initialization**:\n",
    "- Initialize the parameters randomly or with predetermined values, e.g., \\(w = 0.5\\), \\(b = 0.2\\), \\(v = 0.3\\), \\(c = 0.1\\).\n",
    "- Set the learning rate \\(\\alpha\\) (e.g., \\(\\alpha = 0.1\\)).\n",
    "\n",
    "**Training**:\n",
    "1. For each training example, compute the predicted output \\(y\\) using the current network parameters:\n",
    "\n",
    "   \\[y = v \\cdot \\text{sigmoid}(w \\cdot x + b) + c\\]\n",
    "\n",
    "2. Compute the cost function \\(J(\\theta)\\) (e.g., mean squared error) between the predicted output \\(y\\) and the actual target output \\(y_{\\text{target}}\\):\n",
    "\n",
    "   \\[J(\\theta) = \\frac{1}{2}(y - y_{\\text{target}})^2\\]\n",
    "\n",
    "3. Compute the gradients of the cost function with respect to the parameters \\(\\theta\\) using backpropagation. For example:\n",
    "\n",
    "   - \\(\\frac{\\partial J}{\\partial v} = y - y_{\\text{target}}\\)\n",
    "   - \\(\\frac{\\partial J}{\\partial c} = y - y_{\\text{target}}\\)\n",
    "   - \\(\\frac{\\partial J}{\\partial w} = \\frac{\\partial J}{\\partial y} \\cdot \\frac{\\partial y}{\\partial \\text{sigmoid}} \\cdot \\frac{\\partial \\text{sigmoid}}{\\partial(w \\cdot x + b)} \\cdot \\frac{\\partial(w \\cdot x + b)}{\\partial w}\\)\n",
    "   - \\(\\frac{\\partial J}{\\partial b} = \\frac{\\partial J}{\\partial y} \\cdot \\frac{\\partial y}{\\partial \\text{sigmoid}} \\cdot \\frac{\\partial \\text{sigmoid}}{\\partial(w \\cdot x + b)} \\cdot \\frac{\\partial(w \\cdot x + b)}{\\partial b}\\)\n",
    "\n",
    "4. Update the parameters using gradient descent:\n",
    "\n",
    "   - \\(v = v - \\alpha \\cdot \\frac{\\partial J}{\\partial v}\\)\n",
    "   - \\(c = c - \\alpha \\cdot \\frac{\\partial J}{\\partial c}\\)\n",
    "   - \\(w = w - \\alpha \\cdot \\frac{\\partial J}{\\partial w}\\)\n",
    "   - \\(b = b - \\alpha \\cdot \\frac{\\partial J}{\\partial b}\\)\n",
    "\n",
    "Repeat steps 1-4 for multiple iterations and examples until the cost function converges to a minimum, indicating that the network has been trained.\n",
    "\n",
    "The above example illustrates the basic principles of gradient descent for training a simple neural network. In practice, neural networks are often more complex, and libraries like TensorFlow or PyTorch are commonly used for implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080d46b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}