{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b050532",
   "metadata": {},
   "source": [
    "# The Universal Approximation Theorem (UAT)\n",
    "\n",
    "The Universal Approximation Theorem is a fundamental result in the field of neural networks, and it states that a feedforward neural network with a single hidden layer containing a sufficient number of neurons can approximate any continuous function on a compact subset of the real numbers to any desired level of accuracy. Here's a formal statement of the theorem:\n",
    "\n",
    "## Statement of Proof\n",
    "Let $ C(X,\\mathbb{R}^{m})$ denote the set of continuous functions from a subset  $X$ of a Euclidean  $\\mathbb{R}^{n}$ space to a Euclidean space  $\\mathbb{R}^{m}.$ \n",
    "Let $\\sigma$ be any continuous sigmoidal function. Then the finite sums of the form: \n",
    "\n",
    "\\begin{equation}\n",
    "G(\\vec{x})=\\Sigma_{j=1}^N \\alpha_j\\sigma(\\vec{y}_j\\cdot \\vec{x}+\\theta_j),\n",
    "\\end{equation}\n",
    "\n",
    "are dense in $C([0,1]^n)$. In other words, given any $f\\in C([0,1]^n)$ and $\\epsilon >0$, there is a sum $G(\\vec{x})$ of the above form, for which: \n",
    "\n",
    "$$|G(\\vec{x})-f(\\vec{x}) |<\\epsilon,$$\n",
    "\n",
    "for all $\\vec{x}\\in [0,1]^n$.\t\t\n",
    "\n",
    "### Simplification of Statement\n",
    "\n",
    "In simpler terms, the theorem states that, with a single hidden layer, neural networks can approximate a wide range of functions, provided the activation function is continuous and the number of neurons in the hidden layer is sufficiently large. This theorem played a significant role in establishing the practical utility and versatility of neural networks in solving a wide range of problems. It's important to note that more complex networks with multiple hidden layers can provide even more powerful approximations, but the Universal Approximation Theorem specifically focuses on the single hidden layer case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92dc5e67",
   "metadata": {},
   "source": [
    "### Key Points of the Universal Approximation Theorem\n",
    "The UAT is a fundamental result in the field of artificial neural networks (ANN) that states that certain types of neural networks can approximate any continuous function to any desired degree of accuracy. The theorem has several key points:\n",
    "\n",
    "1. **Neural Network Architecture**: The UAT applies to neural networks with at least one hidden layer and a finite number of weights in each layer.\n",
    "\n",
    "2. **Continuous Functions**: The theorem states that any continuous function on a compact subset of R^n can be approximated by a neural network.\n",
    "\n",
    "3. **Activation Functions**: The UAT holds for a variety of activation functions, including the sigmoid function, the hyperbolic tangent function, and the ReLU function.\n",
    "\n",
    "4. **Number of Layers and Neurons**: The UAT can be applied to networks with a finite number of layers and neurons in each layer.\n",
    "\n",
    "5. **Universality**: The UAT implies that a neural network can approximate any continuous function, no matter how complex, as long as the network has enough hidden neurons and layers.\n",
    "\n",
    "The UAT has several implications for the design and capabilities of ANNs. It suggests that a neural network can learn complex patterns and relationships in data as long as certain conditions are fulfilled. It also provides a foundation for understanding how feed-forward networks with one hidden layer can approximate non-linear functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfef15d",
   "metadata": {},
   "source": [
    "### Outline of proof\n",
    "The Universal Approximation Theorem is a fundamental result in neural network theory, stating that a feedforward neural network with a single hidden layer can approximate any continuous function on a compact subset of the real numbers to any desired level of accuracy, given that the activation function satisfies certain conditions. One of these conditions is the Lipschitz condition. Here is an outline of the proof for the Universal Approximation Theorem using the Lipschitz condition:\n",
    "\n",
    "1. **Notation and Definitions**:\n",
    "   - Let $\\varphi(x)$ be a non-constant, bounded, and continuous activation function. We assume it is bounded, i.e., $|\\varphi(x)| \\leq M$ for some constant $M$.\n",
    "   - Consider a feedforward neural network with a single hidden layer and $n$ neurons in the hidden layer.\n",
    "\n",
    "2. **Approximation by Step Functions**:\n",
    "   - First, establish that the network can approximate step functions, which are piecewise constant functions. This is often done by showing that the composition of the activation function $\\varphi$ and linear combinations of inputs can approximate step functions on a compact subset of the real numbers.\n",
    "\n",
    "3. **Construct a Partition of Unity**:\n",
    "   - Show that you can construct a partition of unity on the input space, which means you can find a set of step functions that sum up to 1 for any input $x$. This step is essential to approximate arbitrary continuous functions because it allows you to combine the outputs of different neurons in the hidden layer to approximate any continuous function.\n",
    "\n",
    "4. **Approximation of Continuous Functions**:\n",
    "   - Using the partition of unity, for any continuous function $f(x)$, you can approximate it as follows:\n",
    "     $$f(x) = \\sum_{i=1}^{n} a_i \\cdot \\varphi(w_i \\cdot x + b_i)$$\n",
    "   where $w_i$ are the weights, $b_i$ are the biases, and $a_i$ are constants.\n",
    "\n",
    "5. **Lipschitz Condition**:\n",
    "   - The Lipschitz condition comes into play to ensure that the network's approximation remains bounded. If $\\varphi$ is Lipschitz continuous with a Lipschitz constant $L$, then you can bound the approximation error in terms of $L$.\n",
    "\n",
    "6. **Convergence to the Target Function**:\n",
    "   - Prove that as the number of hidden neurons $n$ increases, the approximation gets better. This usually involves showing that the approximation error can be made arbitrarily small by increasing $n$ and tuning the weights and biases appropriately.\n",
    "\n",
    "7. **Conclusion**:\n",
    "   - Conclude that the single hidden layer neural network with a Lipschitz-continuous activation function $\\varphi$ can approximate any continuous function on a compact subset of the real numbers. This demonstrates the Universal Approximation Theorem using the Lipschitz condition.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
