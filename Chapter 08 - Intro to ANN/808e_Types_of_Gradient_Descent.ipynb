{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80ccff47",
   "metadata": {},
   "source": [
    "\n",
    "**1. Standard Gradient Descent**:\n",
    "   - **How it works**: In standard gradient descent, the parameters (weights and biases) of a model are updated in the direction of the negative gradient of the cost function. This update is performed using the entire training dataset.\n",
    "   - **Strengths**:\n",
    "     - Simple to implement and understand.\n",
    "     - Guarantees convergence to a local minimum for convex functions.\n",
    "   - **Weaknesses**:\n",
    "     - Slow convergence, especially for large datasets or high-dimensional spaces.\n",
    "     - Prone to getting stuck in local minima for non-convex functions.\n",
    "\n",
    "**2. Stochastic Gradient Descent (SGD)**:\n",
    "   - **How it works**: SGD updates the parameters using a single randomly selected training example at each iteration. This introduces randomness and helps escape local minima and speed up convergence.\n",
    "   - **Strengths**:\n",
    "     - Faster convergence, especially for large datasets, due to frequent updates.\n",
    "     - Can escape local minima more easily because of the stochastic nature.\n",
    "   - **Weaknesses**:\n",
    "     - High variance in updates can lead to oscillations and slower convergence.\n",
    "     - May overshoot the optimal solution due to its noisy nature.\n",
    "\n",
    "**3. Mini-Batch Gradient Descent**:\n",
    "   - **How it works**: Mini-batch gradient descent combines the benefits of both standard gradient descent and SGD by updating the parameters using small batches of training examples. It balances computational efficiency and convergence speed.\n",
    "   - **Strengths**:\n",
    "     - Efficient use of parallel processing for computation.\n",
    "     - Strikes a balance between simplicity and speed.\n",
    "   - **Weaknesses**:\n",
    "     - Requires tuning the batch size, impacting convergence and computational efficiency.\n",
    "\n",
    "**4. Momentum Gradient Descent**:\n",
    "   - **How it works**: Momentum GD adds a momentum term to the update, which helps the algorithm navigate through shallow minima, plateaus, and saddle points. It reduces oscillations and noise in parameter updates.\n",
    "   - **Strengths**:\n",
    "     - Accelerated convergence and faster escape from local minima.\n",
    "     - Reduces oscillations in parameter updates.\n",
    "   - **Weaknesses**:\n",
    "     - May overshoot the optimal solution if the momentum coefficient is too high.\n",
    "     - Requires tuning the momentum hyperparameter.\n",
    "\n",
    "**5. Nesterov Accelerated Gradient (NAG)**:\n",
    "   - **How it works**: NAG improves upon momentum GD by estimating the future position of the parameters before computing the gradient. This reduces the overshooting problem associated with momentum.\n",
    "   - **Strengths**:\n",
    "     - Faster convergence compared to vanilla momentum GD.\n",
    "     - Reduces overshooting, making it more stable.\n",
    "   - **Weaknesses**:\n",
    "     - Still requires tuning the momentum hyperparameter.\n",
    "\n",
    "**6. Adagrad**:\n",
    "   - **How it works**: Adagrad adapts the learning rates for each parameter based on the historical gradients. It assigns larger updates to infrequent parameters and smaller updates to frequently occurring ones.\n",
    "   - **Strengths**:\n",
    "     - Well-suited for sparse data and features.\n",
    "     - Automatic adaptation of learning rates.\n",
    "   - **Weaknesses**:\n",
    "     - Learning rates decrease aggressively for frequently occurring features, potentially causing premature convergence.\n",
    "     - Accumulated squared gradients in the denominator can become very large.\n",
    "\n",
    "**7. RMSprop**:\n",
    "   - **How it works**: RMSprop addresses Adagrad's aggressive learning rate reduction by using a moving average of squared gradients, preventing learning rates from becoming too small.\n",
    "   - **Strengths**:\n",
    "     - Handles sparse data better than Adagrad.\n",
    "     - Effective adaptive learning rates.\n",
    "   - **Weaknesses**:\n",
    "     - Requires tuning the smoothing parameter to balance learning rate adaptation.\n",
    "\n",
    "**8. Adam (Adaptive Moment Estimation)**:\n",
    "   - **How it works**: Adam combines momentum and RMSprop by maintaining both the moving average of gradients and the moving average of squared gradients. It adapts learning rates and is widely used in practice.\n",
    "   - **Strengths**:\n",
    "     - Often converges faster and performs well across various tasks.\n",
    "     - Adaptive learning rates and momentum.\n",
    "   - **Weaknesses**:\n",
    "     - Requires tuning multiple hyperparameters, including the momentum coefficient and the smoothing parameter.\n",
    "     - May converge to suboptimal solutions for certain non-convex functions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9546c20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
