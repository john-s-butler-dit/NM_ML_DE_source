{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6a2059e",
   "metadata": {},
   "source": [
    "# Two Variable Newton Raphson"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7de27b",
   "metadata": {},
   "source": [
    "The Newton-Raphson method can be extended to optimize functions with two or more variables. In this example, we'll illustrate the Newton-Raphson method using a cost function with two variables $\\theta_1$ and $\\theta_2$. The goal is to minimize the cost function by finding the optimal values of these variables.\n",
    "\n",
    "Suppose we have the following cost function:\n",
    "\n",
    "$$J(\\theta_1, \\theta_2) = \\theta_1^2 + \\theta_2^2 - 4\\theta_1 - 4\\theta_2$$\n",
    "\n",
    "We want to find the minimum of this cost function using the Newton-Raphson method. First, we need to compute the gradient vector and the Hessian matrix. The gradient vector $\\nabla J$ is given by:\n",
    "\n",
    "$$\n",
    "\\nabla J = \\begin{bmatrix}\n",
    "2\\theta_1 - 4 \\\\\n",
    "2\\theta_2 - 4\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The Hessian matrix $\\mathbf{H}$ is given by:\n",
    "\n",
    "$$\n",
    "\\mathbf{H} = \\begin{bmatrix}\n",
    "2 & 0 \\\\\n",
    "0 & 2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Now, we can apply the Newton-Raphson update rule:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\theta_1 \\\\\n",
    "\\theta_2\n",
    "\\end{bmatrix}_{k+1} = \\begin{bmatrix}\n",
    "\\theta_1 \\\\\n",
    "\\theta_2\n",
    "\\end{bmatrix}_{k} - \\mathbf{H}^{-1} \\nabla J\n",
    "$$\n",
    "\n",
    "Let's perform a few iterations of the Newton-Raphson method with an initial guess:\n",
    "\n",
    "**Initialization**:\n",
    "- $\\theta_1 = 2.0$\n",
    "- $\\theta_2 = 2.0$\n",
    "\n",
    "**Iteration 1**:\n",
    "1. Calculate the gradient and the Hessian matrix at the current parameters:\n",
    "\n",
    "   $\\nabla J = \\begin{bmatrix}\n",
    "   2 \\cdot 2 - 4 \\\\\n",
    "   2 \\cdot 2 - 4\n",
    "   \\end{bmatrix} = \\begin{bmatrix}\n",
    "   0 \\\\\n",
    "   0\n",
    "   \\end{bmatrix}$\n",
    "\n",
    "   $\\mathbf{H} = \\begin{bmatrix}\n",
    "   2 & 0 \\\\\n",
    "   0 & 2\n",
    "   \\end{bmatrix}$\n",
    "\n",
    "2. Update the parameters using the Newton-Raphson formula:\n",
    "\n",
    "   $\\begin{bmatrix}\n",
    "   \\theta_1 \\\\\n",
    "   \\theta_2\n",
    "   \\end{bmatrix}_{k+1} = \\begin{bmatrix}\n",
    "   \\theta_1 \\\\\n",
    "   \\theta_2\n",
    "   \\end{bmatrix}_{k} - \\mathbf{H}^{-1} \\nabla J = \\begin{bmatrix}\n",
    "   2 \\\\\n",
    "   2\n",
    "   \\end{bmatrix} - \\begin{bmatrix}\n",
    "   0 \\\\\n",
    "   0\n",
    "   \\end{bmatrix} = \\begin{bmatrix}\n",
    "   2 \\\\\n",
    "   2\n",
    "   \\end{bmatrix}$\n",
    "\n",
    "The algorithm converges after the first iteration because the gradient is zero, indicating that we've reached a minimum.\n",
    "\n",
    "In this example, the Newton-Raphson method finds the optimal values of $\\theta_1$ and $\\theta_2$ that minimize the cost function quickly because it's a simple quadratic function. In more complex functions, it may require multiple iterations to converge to the minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe0e9e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
