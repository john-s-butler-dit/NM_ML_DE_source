{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d50c3dd",
   "metadata": {},
   "source": [
    "## Back Progagation ReLU three inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7e5896",
   "metadata": {},
   "source": [
    "Certainly! Let's walk through an example of using gradient descent to train a simple artificial neural network (ANN) with one input node, one hidden node (using the ReLU activation function), and one output node. We will use a basic regression problem as an example.\n",
    "\n",
    "**Problem Statement**:\n",
    "We want to train a neural network to predict the output (y) based on a single input feature (x). The network architecture is as follows:\n",
    "\n",
    "1. Input layer: One neuron with input \\(x\\).\n",
    "2. Hidden layer: One neuron with weight \\(w\\) and bias \\(b\\), applying the ReLU activation function.\n",
    "3. Output layer: One neuron with weight \\(v\\) and bias \\(c\\), representing the predicted output.\n",
    "\n",
    "The network's output \\(y\\) can be expressed as:\n",
    "\n",
    "\\[y = v \\cdot \\text{ReLU}(w \\cdot x + b) + c\\]\n",
    "\n",
    "**Data**:\n",
    "Let's assume we have a dataset with input-output pairs (x, y) for training:\n",
    "\n",
    "\\[\n",
    "\\begin{align*}\n",
    "(x_1, y_1) &= (2.0, 2.0) \\\\\n",
    "(x_2, y_2) &= (3.0, 3.0) \\\\\n",
    "(x_3, y_3) &= (4.0, 4.0) \\\\\n",
    "\\end{align*}\n",
    "\\]\n",
    "\n",
    "**Initialization**:\n",
    "- Initialize the parameters randomly or with predetermined values:\n",
    "  - \\(w = 0.5\\)\n",
    "  - \\(b = 0.2\\)\n",
    "  - \\(v = 0.3\\)\n",
    "  - \\(c = 0.1\\)\n",
    "- Set the learning rate \\(\\alpha\\) (e.g., \\(\\alpha = 0.1\\)).\n",
    "\n",
    "**Training**:\n",
    "We'll use gradient descent to update the network parameters in order to minimize the mean squared error (MSE) cost function:\n",
    "\n",
    "\\[J(\\theta) = \\frac{1}{2N} \\sum_{i=1}^N (y_i - y_{\\text{target},i})^2\\]\n",
    "\n",
    "where \\(N\\) is the number of training examples, \\(y_i\\) is the predicted output for the \\(i\\)-th example, and \\(y_{\\text{target},i}\\) is the target output for the \\(i\\)-th example.\n",
    "\n",
    "Here's how we update the parameters in each training iteration:\n",
    "\n",
    "1. For each training example (x, y), calculate the predicted output \\(y\\) using the current network parameters:\n",
    "\n",
    "   \\[y = v \\cdot \\text{ReLU}(w \\cdot x + b) + c\\]\n",
    "\n",
    "2. Calculate the cost for this example:\n",
    "\n",
    "   \\[J(\\theta) = \\frac{1}{2}(y - y_{\\text{target}})^2\\]\n",
    "\n",
    "3. Compute the gradients of the cost with respect to the parameters:\n",
    "\n",
    "   - \\(\\frac{\\partial J}{\\partial v} = y - y_{\\text{target}}\\)\n",
    "   - \\(\\frac{\\partial J}{\\partial c} = y - y_{\\text{target}}\\)\n",
    "   - \\(\\frac{\\partial J}{\\partial w} = \\frac{\\partial J}{\\partial y} \\cdot \\frac{\\partial y}{\\partial \\text{ReLU}} \\cdot \\frac{\\partial \\text{ReLU}}{\\partial(w \\cdot x + b)} \\cdot \\frac{\\partial(w \\cdot x + b)}{\\partial w}\\)\n",
    "   - \\(\\frac{\\partial J}{\\partial b} = \\frac{\\partial J}{\\partial y} \\cdot \\frac{\\partial y}{\\partial \\text{ReLU}} \\cdot \\frac{\\partial \\text{ReLU}}{\\partial(w \\cdot x + b)} \\cdot \\frac{\\partial(w \\cdot x + b)}{\\partial b}\\)\n",
    "\n",
    "4. Update the parameters using gradient descent:\n",
    "\n",
    "   - \\(v = v - \\alpha \\cdot \\frac{\\partial J}{\\partial v}\\)\n",
    "   - \\(c = c - \\alpha \\cdot \\frac{\\partial J}{\\partial c}\\)\n",
    "   - \\(w = w - \\alpha \\cdot \\frac{\\partial J}{\\partial w}\\)\n",
    "   - \\(b = b - \\alpha \\cdot \\frac{\\partial J}{\\partial b}\\)\n",
    "\n",
    "Repeat steps 1-4 for a specified number of training iterations or until the cost converges to a minimum. In each iteration, you use the gradients to adjust the parameters in the direction that minimizes the cost function, thus training the neural network.\n",
    "\n",
    "This example illustrates a basic gradient descent approach for training a simple neural network with ReLU activation. In practice, real-world scenarios involve larger networks, more data, and libraries like TensorFlow or PyTorch to handle the training process efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "881fd32e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated w: 0.5924\n",
      "Updated b: 0.2462\n",
      "Updated v: 0.4848\n",
      "Updated c: 0.254\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the neural network architecture\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Initialize the network parameters\n",
    "w = 0.5\n",
    "b = 0.2\n",
    "v = 0.3\n",
    "c = 0.1\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Define the training data\n",
    "x = 2.0\n",
    "y_target = 2.0\n",
    "\n",
    "# Forward pass\n",
    "z = w * x + b\n",
    "h = relu(z)\n",
    "y = v * h + c\n",
    "\n",
    "# Compute the cost\n",
    "cost = 0.5 * (y - y_target)**2\n",
    "\n",
    "# Compute the gradients\n",
    "dJ_dy = y - y_target\n",
    "dJ_dv = dJ_dy * h\n",
    "dJ_dc = dJ_dy\n",
    "dJ_dh = dJ_dy * v\n",
    "dJ_dz = dJ_dh if z > 0 else 0\n",
    "dJ_dw = dJ_dz * x\n",
    "dJ_db = dJ_dz\n",
    "\n",
    "# Update the parameters using gradient descent\n",
    "v -= learning_rate * dJ_dv\n",
    "c -= learning_rate * dJ_dc\n",
    "w -= learning_rate * dJ_dw\n",
    "b -= learning_rate * dJ_db\n",
    "\n",
    "# Print the updated parameters\n",
    "print(f\"Updated w: {w}\")\n",
    "print(f\"Updated b: {b}\")\n",
    "print(f\"Updated v: {v}\")\n",
    "print(f\"Updated c: {c}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec4521b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
