{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd379cd3",
   "metadata": {},
   "source": [
    "## Three Layer Feed Forward ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a3efbf",
   "metadata": {},
   "source": [
    "The mathematical calculations for a simple artificial neural network (ANN) with two hidden layers, each having three ReLU (Rectified Linear Unit) nodes, and an output layer with a Sigmoid activation function. Let's denote the inputs as $x_1$, $x_2$, and $x_3$, the weights for the connections between the input layer and the first hidden layer as $w_{ij}^{(1)}$ (where $i$ represents the input node, and $j$ represents the hidden node), the biases for the first hidden layer as $b_j^{(1)}$, the weights for the connections between the first hidden layer and the second hidden layer as $w_{jk}^{(2)}$, the biases for the second hidden layer as $b_k^{(2)}$, the weights for the connections between the second hidden layer and the output layer as $w_{kl}^{(3)}$, and the bias for the output layer as $b_l^{(3)}$.\n",
    "\n",
    "The calculations for the nodes in the first hidden layer ($h_1^{(1)}$, $h_2^{(1)}$, and $h_3^{(1)}$) are as follows:\n",
    "\n",
    "$$\n",
    "h_1^{(1)} = \\text{ReLU}(w_{11}^{(1)}x_1 + w_{21}^{(1)}x_2 + w_{31}^{(1)}x_3 + b_1^{(1)})\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_2^{(1)} = \\text{ReLU}(w_{12}^{(1)}x_1 + w_{22}^{(1)}x_2 + w_{32}^{(1)}x_3 + b_2^{(1)})\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_3^{(1)} = \\text{ReLU}(w_{13}^{(1)}x_1 + w_{23}^{(1)}x_2 + w_{33}^{(1)}x_3 + b_3^{(1)})\n",
    "$$\n",
    "\n",
    "The ReLU function is defined as $\\text{ReLU}(x) = \\max(0, x)$, meaning that it outputs the input value if it is positive and zero otherwise.\n",
    "\n",
    "The calculations for the nodes in the second hidden layer ($h_1^{(2)}$, $h_2^{(2)}$, and $h_3^{(2)}$) are similar:\n",
    "\n",
    "$$\n",
    "h_1^{(2)} = \\text{ReLU}(w_{11}^{(2)}h_1^{(1)} + w_{21}^{(2)}h_2^{(1)} + w_{31}^{(2)}h_3^{(1)} + b_1^{(2)})\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_2^{(2)} = \\text{ReLU}(w_{12}^{(2)}h_1^{(1)} + w_{22}^{(2)}h_2^{(1)} + w_{32}^{(2)}h_3^{(1)} + b_2^{(2)})\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_3^{(2)} = \\text{ReLU}(w_{13}^{(2)}h_1^{(1)} + w_{23}^{(2)}h_2^{(1)} + w_{33}^{(2)}h_3^{(1)} + b_3^{(2)})\n",
    "$$\n",
    "\n",
    "Finally, the calculation for the output node ($y$) with the Sigmoid activation function is:\n",
    "\n",
    "$$\n",
    "y = \\text{Sigmoid}(w_{11}^{(3)}h_1^{(2)} + w_{21}^{(3)}h_2^{(2)} + w_{31}^{(3)}h_3^{(2)} + b^{(3)})\n",
    "$$\n",
    "\n",
    "Where $\\text{Sigmoid}(x) = \\frac{1}{1 + e^{-x}}$ is the sigmoid function that squashes the output value between 0 and 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9d0649d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "[[2.86873055e-02]\n",
      " [3.81899579e-05]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the neural network architecture\n",
    "input_size = 3  # Number of input features\n",
    "hidden_size1 = 3  # Number of nodes in the hidden layer\n",
    "hidden_size2 = 3  # Number of nodes in the hidden layer\n",
    "output_size = 1  # Number of output nodes\n",
    "\n",
    "# Initialize random weights and biases\n",
    "np.random.seed(0)\n",
    "input_layer_size = input_size\n",
    "hidden_layer1_size = hidden_size1\n",
    "hidden_layer2_size = hidden_size2\n",
    "output_layer_size = output_size\n",
    "\n",
    "# Initialize weights and biases with random values\n",
    "weights_input_hidden1 = np.random.randn(input_layer_size, hidden_layer1_size)\n",
    "bias_hidden1 = np.zeros((1, hidden_layer1_size))\n",
    "# Initialize weights and biases with random values\n",
    "weights_input_hidden2 = np.random.randn(hidden_layer1_size, hidden_layer2_size)\n",
    "bias_hidden2 = np.zeros((1, hidden_layer2_size))\n",
    "weights_hidden_output = np.random.randn(hidden_layer2_size, output_layer_size)\n",
    "bias_output = np.zeros((1, output_size))\n",
    "\n",
    "# Define the sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Define the ReLU activation function\n",
    "def ReLU(x):\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "\n",
    "# Define the forward propagation function\n",
    "def forward_propagation(X):\n",
    "    # Calculate the values for the hidden layer 1\n",
    "    hidden_input1 = np.dot(X.T, weights_input_hidden1) + bias_hidden1\n",
    "    hidden_output1 = ReLU(hidden_input1)\n",
    "\n",
    "    # Calculate the values for the hidden layer 2\n",
    "    hidden_input2 = np.dot(hidden_output1, weights_input_hidden2) + bias_hidden2\n",
    "    hidden_output2 = ReLU(hidden_input2)\n",
    "\n",
    "    # Calculate the values for the output layer\n",
    "    output_input = np.dot(hidden_output2, weights_hidden_output) + bias_output\n",
    "    output = sigmoid(output_input)\n",
    "    \n",
    "    return hidden_input1, hidden_output1,hidden_input2, hidden_output2, output_input, output\n",
    "\n",
    "# Example input data\n",
    "X = np.array([[0, 0, 1], [0, 1, 0]]).T  # Two input samples\n",
    "\n",
    "# Perform forward propagation\n",
    "hidden_input1, hidden_output1,hidden_input2, hidden_output2,  output_input, output = forward_propagation(X)\n",
    "\n",
    "# Print the output\n",
    "print(\"Output:\")\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3708a82d",
   "metadata": {},
   "source": [
    "## Outline of the Maths for an ANN\n",
    "\n",
    "1. Input Layer:\n",
    "   - Input data: X\n",
    "\n",
    "2. First Hidden Layer:\n",
    "   - Number of nodes: 3\n",
    "   - Weight matrix: W1 (3x3)\n",
    "   - Bias vector: b1 (1x3)\n",
    "   - Activation function: ReLU (Rectified Linear Unit)\n",
    "\n",
    "The output of the first hidden layer (Z1) is calculated as follows:\n",
    "\n",
    "   Z1 = X * W1 + b1\n",
    "\n",
    "   Where:\n",
    "   - X is the input data (a row vector),\n",
    "   - W1 is the weight matrix for the first hidden layer,\n",
    "   - b1 is the bias vector for the first hidden layer,\n",
    "   - \"*\" represents matrix multiplication.\n",
    "\n",
    "3. Apply ReLU activation to Z1:\n",
    "\n",
    "   A1 = ReLU(Z1)\n",
    "\n",
    "   A1 is the output of the first hidden layer.\n",
    "\n",
    "4. Second Hidden Layer:\n",
    "   - Number of nodes: 3\n",
    "   - Weight matrix: W2 (3x3)\n",
    "   - Bias vector: b2 (1x3)\n",
    "   - Activation function: ReLU\n",
    "\n",
    "The output of the second hidden layer (Z2) is calculated in the same way as the first hidden layer:\n",
    "\n",
    "   Z2 = A1 * W2 + b2\n",
    "\n",
    "5. Apply ReLU activation to Z2:\n",
    "\n",
    "   A2 = ReLU(Z2)\n",
    "\n",
    "   A2 is the output of the second hidden layer.\n",
    "\n",
    "6. Output Layer:\n",
    "   - Number of nodes: 1\n",
    "   - Weight matrix: W3 (3x1)\n",
    "   - Bias vector: b3 (1x1)\n",
    "   - Activation function: Sigmoid\n",
    "\n",
    "The output of the output layer (Z3) is calculated as:\n",
    "\n",
    "   Z3 = A2 * W3 + b3\n",
    "\n",
    "7. Apply the Sigmoid activation to Z3 to get the final output (Y):\n",
    "\n",
    "   Y = Sigmoid(Z3)\n",
    "\n",
    "This is the forward pass of the network, and it calculates the output based on the given input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ab465e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
