{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "869e8cf3",
   "metadata": {},
   "source": [
    "## General Feed Forward for an ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2ef9a5",
   "metadata": {},
   "source": [
    "The mathematical calculations for a deep artificial neural network (ANN) with $n$ hidden layers and one output layer can be generalized as follows. Let's denote the inputs as $x_1, x_2, \\ldots, x_m$ (where $m$ is the number of input features), and the weights and biases for each layer as $w^{(k)}$ and $b^{(k)}$ respectively, where $k$ represents the layer index.\n",
    "\n",
    "The calculations for the nodes in each hidden layer and the output layer are as follows:\n",
    "\n",
    "1. For the first hidden layer ($k = 1$):\n",
    "   $$\n",
    "   h^{(1)}_j = \\text{ReLU}\\left(\\sum_{i=1}^{m} w^{(1)}_{ij}x_i + b^{(1)}_j\\right) \\quad \\text{for } j = 1, 2, \\ldots, n_1\n",
    "   $$\n",
    "\n",
    "2. For subsequent hidden layers ($k = 2, 3, \\ldots, n$):\n",
    "   $$\n",
    "   h^{(k)}_j = \\text{ReLU}\\left(\\sum_{i=1}^{n_{k-1}} w^{(k)}_{ij}h^{(k-1)}_i + b^{(k)}_j\\right) \\quad \\text{for } j = 1, 2, \\ldots, n_k\n",
    "   $$\n",
    "\n",
    "3. For the output layer ($k = n+1$):\n",
    "   $$\n",
    "   y = \\text{Sigmoid}\\left(\\sum_{i=1}^{n_n} w^{(n+1)}_i h^{(n)}_i + b^{(n+1)}\\right)\n",
    "   $$\n",
    "\n",
    "In the above equations:\n",
    "\n",
    "- $n$ is the total number of hidden layers.\n",
    "- $n_k$ represents the number of nodes in the $k$-th hidden layer.\n",
    "- $\\text{ReLU}(x) = \\max(0, x)$ is the Rectified Linear Unit activation function for hidden layers.\n",
    "- $\\text{Sigmoid}(x) = \\frac{1}{1 + e^{-x}}$ is the Sigmoid activation function for the output layer.\n",
    "\n",
    "During training, the weights ($w^{(k)}$) and biases ($b^{(k)}$) are adjusted using optimization algorithms like gradient descent to minimize the difference between the predicted output $y$ and the actual target values.\n",
    "\n",
    "This generalizes the calculations for a deep neural network with $n$ hidden layers. The specific values of $n_k$ and the weights and biases would depend on the architecture of your neural network and the specific problem you're solving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c33f743d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "[[0.51236918]\n",
      " [0.63539938]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the neural network architecture\n",
    "input_size = 4  # Number of input features\n",
    "hidden_size = 5  # Number of nodes in the hidden layer\n",
    "output_size = 1  # Number of output nodes\n",
    "\n",
    "# Initialize random weights and biases\n",
    "np.random.seed(0)\n",
    "input_layer_size = input_size\n",
    "hidden_layer_size = hidden_size\n",
    "output_layer_size = output_size\n",
    "\n",
    "# Initialize weights and biases with random values\n",
    "weights_input_hidden = np.random.randn(input_layer_size, hidden_layer_size)\n",
    "bias_hidden = np.zeros((1, hidden_layer_size))\n",
    "weights_hidden_output = np.random.randn(hidden_layer_size, output_layer_size)\n",
    "bias_output = np.zeros((1, output_size))\n",
    "\n",
    "# Define the sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Define the ReLU activation function\n",
    "def ReLU(x):\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "\n",
    "# Define the forward propagation function\n",
    "def forward_propagation(X):\n",
    "    # Calculate the values for the hidden layer\n",
    "    hidden_input = np.dot(X.T, weights_input_hidden) + bias_hidden\n",
    "    hidden_output = ReLU(hidden_input)\n",
    "    \n",
    "    # Calculate the values for the output layer\n",
    "    output_input = np.dot(hidden_output, weights_hidden_output) + bias_output\n",
    "    output = sigmoid(output_input)\n",
    "    \n",
    "    return hidden_input, hidden_output, output_input, output\n",
    "\n",
    "# Example input data\n",
    "X = np.array([[0, 0, 1, 1], [0, 1, 0, 1]]).T  # Two input samples\n",
    "\n",
    "# Perform forward propagation\n",
    "hidden_input, hidden_output, output_input, output = forward_propagation(X)\n",
    "\n",
    "# Print the output\n",
    "print(\"Output:\")\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80a4b2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of Layer 1:\n",
      "[[0.61720885 0.95018557 0.63549859 0.60700562 0.39885637]\n",
      " [0.34443241 0.92013388 0.41180333 0.55227054 0.39090796]]\n",
      "Output of Layer 2:\n",
      "[[0.81859503 0.31702007 0.57683967 0.3962107 ]\n",
      " [0.86187865 0.24380832 0.5150207  0.42459047]]\n",
      "Output of Layer 3:\n",
      "[[0.25982904 0.22612967 0.09002841]\n",
      " [0.23800412 0.20950305 0.09584056]]\n",
      "Output of Layer 4:\n",
      "[[0.40077544]\n",
      " [0.40814945]]\n",
      "Final Output:\n",
      "[[0.40077544]\n",
      " [0.40814945]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the neural network architecture\n",
    "input_size = 4  # Number of input features\n",
    "hidden_layer_sizes = [5, 4, 3]  # List of hidden layer sizes\n",
    "output_size = 1  # Number of output nodes\n",
    "\n",
    "# Initialize random weights and biases\n",
    "np.random.seed(0)\n",
    "layer_sizes = [input_size] + hidden_layer_sizes + [output_size]\n",
    "num_layers = len(layer_sizes)\n",
    "\n",
    "# Initialize weights and biases with random values\n",
    "weights = [np.random.randn(layer_sizes[i], layer_sizes[i+1]) for i in range(num_layers - 1)]\n",
    "biases = [np.zeros((1, layer_sizes[i+1])) for i in range(num_layers - 1)]\n",
    "\n",
    "# Define the sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "# Define the forward propagation function\n",
    "def forward_propagation(X):\n",
    "    layer_outputs = []\n",
    "    for i in range(num_layers - 1):\n",
    "        if i == 0:\n",
    "            layer_input = X.T\n",
    "        else:\n",
    "            layer_input = layer_outputs[-1]\n",
    "        \n",
    "        # Calculate the values for the current layer\n",
    "        layer_input = np.dot(layer_input, weights[i]) + biases[i]\n",
    "        layer_output = sigmoid(layer_input)\n",
    "        layer_outputs.append(layer_output)\n",
    "    \n",
    "    return layer_outputs\n",
    "\n",
    "# Example input data\n",
    "X = np.array([[0, 0, 1, 1], [0, 1, 0, 1]]).T  # Two input samples\n",
    "\n",
    "# Perform forward propagation\n",
    "output_layers = forward_propagation(X)\n",
    "\n",
    "# Print the output of each layer\n",
    "for i, output in enumerate(output_layers):\n",
    "    print(f\"Output of Layer {i + 1}:\\n{output}\")\n",
    "\n",
    "# The final output is in output_layers[-1]\n",
    "print(\"Final Output:\")\n",
    "print(output_layers[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c3dc80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
