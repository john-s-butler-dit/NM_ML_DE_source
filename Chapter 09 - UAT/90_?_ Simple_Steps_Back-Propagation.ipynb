{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "387bb025",
   "metadata": {},
   "source": [
    "# Simple Steps Back-Propagation\n",
    "\n",
    "The  steps for the backpropagation of a simple neural network with one input layer, one hidden layer with three ReLU nodes, and one linear output. Let's denote:\n",
    "\n",
    "- $x$ as the input,\n",
    "- $h_1, h_2, h_3$ as the hidden layer nodes with ReLU activation,\n",
    "- $y$ as the output,\n",
    "- $w$ as the weights, and\n",
    "- $b$ as the bias.\n",
    "\n",
    "Assuming a Mean Squared Error (MSE) loss function, the steps for backpropagation are as follows:\n",
    "\n",
    "1. **Forward Pass:**\n",
    "   - Compute the weighted sum and apply the ReLU activation for the hidden layer nodes:\n",
    "     $$\n",
    "     \\begin{align*}\n",
    "     a_1 &= w_{11}^{(1)}x + b_{1}^{(1)} \\\\\n",
    "     h_1 &= \\max(0, a_1) \\\\\n",
    "     a_2 &= w_{21}^{(1)}x + b_{2}^{(1)} \\\\\n",
    "     h_2 &= \\max(0, a_2) \\\\\n",
    "     a_3 &= w_{31}^{(1)}x + b_{3}^{(1)} \\\\\n",
    "     h_3 &= \\max(0, a_3) \\\\\n",
    "     \\end{align*}\n",
    "     $$\n",
    "   - Compute the weighted sum for the output node:\n",
    "     $$\n",
    "     \\begin{align*}\n",
    "     a &= w_{1}^{(2)}h_1 + w_{2}^{(2)}h_2 + w_{3}^{(2)}h_3 + b^{(2)} \\\\\n",
    "     y &= a\n",
    "     \\end{align*}\n",
    "     $$\n",
    "\n",
    "2. **Compute the Loss:**\n",
    "   - Compute the loss using the MSE formula:\n",
    "     $$\n",
    "     L = \\frac{1}{2}(y_{\\text{true}} - y_{\\text{predicted}})^2\n",
    "     $$\n",
    "\n",
    "3. **Backward Pass:**\n",
    "   - Compute the gradient of the loss with respect to the output node:\n",
    "     $$\n",
    "     \\frac{\\partial L}{\\partial a} = y_{\\text{predicted}} - y_{\\text{true}}\n",
    "     $$\n",
    "   - Update the weights and biases for the output layer:\n",
    "     $$\n",
    "     \\begin{align*}\n",
    "     \\frac{\\partial L}{\\partial w_{i}^{(2)}} &= \\frac{\\partial L}{\\partial a} \\cdot h_i \\\\\n",
    "     \\frac{\\partial L}{\\partial b^{(2)}} &= \\frac{\\partial L}{\\partial a}\n",
    "     \\end{align*}\n",
    "     $$\n",
    "   - Propagate the gradient through the ReLU activation for the hidden layer nodes:\n",
    "     $$\n",
    "     \\begin{align*}\n",
    "     \\frac{\\partial L}{\\partial a_i} &= \n",
    "     \\begin{cases}\n",
    "     \\frac{\\partial L}{\\partial a_i}, & \\text{if } a_i > 0 \\\\\n",
    "     0, & \\text{otherwise}\n",
    "     \\end{cases}\n",
    "     \\end{align*}\n",
    "     $$\n",
    "   - Update the weights and biases for the hidden layer:\n",
    "     $$\n",
    "     \\begin{align*}\n",
    "     \\frac{\\partial L}{\\partial w_{ij}^{(1)}} &= \\frac{\\partial L}{\\partial a_j} \\cdot x_i, \\quad \\text{where } j \\in \\{1,2,3\\} \\\\\n",
    "     \\frac{\\partial L}{\\partial b^{(1)}_j} &= \\frac{\\partial L}{\\partial a_j}\n",
    "     \\end{align*}\n",
    "     $$\n",
    "\n",
    "These equations give you the gradients needed to perform gradient descent and update the weights and biases in the network. The learning rate and the number of iterations would be additional parameters to consider when implementing this in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731c4786",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
