{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b408a6d3",
   "metadata": {},
   "source": [
    "To solve the initial value problem y' = 0.5y with the initial condition y(0) = 1 using a two-layer Artificial Neural Network (ANN) with ReLU activation function in PyTorch, while incorporating the derivative in the loss function, you can frame it as a boundary value problem (BVP). Here's how you can implement it:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the neural network model\n",
    "class TwoLayerNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TwoLayerNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(1, 16)  # Single input feature, 16 hidden units\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(16, 1)  # 16 hidden units, single output\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Define the differential equation y' = 0.5y\n",
    "def differential_equation(y, t):\n",
    "    return 0.5 * y\n",
    "\n",
    "# Define a custom loss function incorporating the derivative\n",
    "def custom_loss(y_pred, y_true, t):\n",
    "    dydt_pred = torch.autograd.grad(outputs=y_pred, inputs=t, grad_outputs=torch.ones_like(y_pred), create_graph=True)[0]\n",
    "    loss_mse = nn.MSELoss()(y_pred, y_true)\n",
    "    loss_derivative = nn.MSELoss()(dydt_pred, 0.5 * y_pred)  # Incorporate the derivative in the loss\n",
    "    return loss_mse + loss_derivative\n",
    "\n",
    "# Initial condition\n",
    "y0 = 1.0\n",
    "\n",
    "# Time parameters\n",
    "t0 = 0.0\n",
    "t_final = 5.0  # Adjust the final time as needed\n",
    "delta_t = 0.1  # Adjust the time step as needed\n",
    "\n",
    "# Number of time steps\n",
    "num_steps = int((t_final - t0) / delta_t)\n",
    "\n",
    "# Create tensors for time and solution\n",
    "t = torch.linspace(t0, t_final, num_steps + 1, requires_grad=True).reshape(-1, 1)\n",
    "y_true = y0 * torch.exp(0.5 * t)  # True solution for comparison\n",
    "\n",
    "# Instantiate the neural network, loss function, and optimizer\n",
    "model = TwoLayerNN()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training the neural network to approximate the solution\n",
    "for epoch in range(1000):  # You can adjust the number of epochs as needed\n",
    "    # Predict y values using the neural network\n",
    "    y_pred = model(t)\n",
    "    \n",
    "    # Compute the custom loss function\n",
    "    loss = custom_loss(y_pred, y_true, t)\n",
    "    \n",
    "    # Perform backpropagation and optimization step\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print the loss at every 100 epochs\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item():.6f}')\n",
    "\n",
    "# Evaluate the trained model at t_final\n",
    "predicted_y = model(torch.tensor([[t_final]], requires_grad=True))\n",
    "print(f'Predicted y({t_final}) = {predicted_y.item():.4f}')\n",
    "```\n",
    "\n",
    "In this code, we define a two-layer neural network with ReLU activation functions and incorporate the derivative into the custom loss function. The loss function combines Mean Squared Error (MSE) loss for the predicted y values and the derivative with respect to time t. This setup allows you to solve the BVP by finding the function y(t) that satisfies both the differential equation and the initial condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00a4f661",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Boolean value of Tensor with more than one value is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 55\u001b[0m\n\u001b[1;32m     52\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model(t)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Compute the custom loss function\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m loss \u001b[38;5;241m=\u001b[39m custom_loss(y_pred, y_true, t)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Perform backpropagation and optimization step\u001b[39;00m\n\u001b[1;32m     58\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "Cell \u001b[0;32mIn[4], line 26\u001b[0m, in \u001b[0;36mcustom_loss\u001b[0;34m(y_pred, y_true, t)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcustom_loss\u001b[39m(y_pred, y_true, t):\n\u001b[1;32m     25\u001b[0m     dydt_pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(outputs\u001b[38;5;241m=\u001b[39my_pred, inputs\u001b[38;5;241m=\u001b[39mt, grad_outputs\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mones_like(y_pred), create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 26\u001b[0m     loss_mse \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss(y_pred, y_true)\n\u001b[1;32m     27\u001b[0m     loss_derivative \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss(dydt_pred, \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m y_pred)  \u001b[38;5;66;03m# Incorporate the derivative in the loss\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mse \u001b[38;5;241m+\u001b[39m loss_derivative\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/Numerical_and_ML_Methods_for_DE/lib/python3.11/site-packages/torch/nn/modules/loss.py:532\u001b[0m, in \u001b[0;36mMSELoss.__init__\u001b[0;34m(self, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, size_average\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, reduce\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, reduction: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 532\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(size_average, reduce, reduction)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/Numerical_and_ML_Methods_for_DE/lib/python3.11/site-packages/torch/nn/modules/loss.py:23\u001b[0m, in \u001b[0;36m_Loss.__init__\u001b[0;34m(self, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction \u001b[38;5;241m=\u001b[39m reduction\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/Numerical_and_ML_Methods_for_DE/lib/python3.11/site-packages/torch/nn/_reduction.py:35\u001b[0m, in \u001b[0;36mlegacy_get_string\u001b[0;34m(size_average, reduce, emit_warning)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     reduce \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mand\u001b[39;00m reduce:\n\u001b[1;32m     36\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m reduce:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Boolean value of Tensor with more than one value is ambiguous"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the neural network model\n",
    "class TwoLayerNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TwoLayerNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(1, 16)  # Single input feature, 16 hidden units\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(16, 1)  # 16 hidden units, single output\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Define the differential equation y' = 0.5y\n",
    "def differential_equation(y, t):\n",
    "    return 0.5 * y\n",
    "\n",
    "# Define a custom loss function incorporating the derivative\n",
    "def custom_loss(y_pred, y_true, t):\n",
    "    dydt_pred = torch.autograd.grad(outputs=y_pred, inputs=t, grad_outputs=torch.ones_like(y_pred), create_graph=True)[0]\n",
    "    loss_mse = nn.MSELoss(y_pred, y_true)\n",
    "    loss_derivative = nn.MSELoss(dydt_pred, 0.5 * y_pred)  # Incorporate the derivative in the loss\n",
    "    return loss_mse + loss_derivative\n",
    "\n",
    "# Initial condition\n",
    "y0 = 1.0\n",
    "\n",
    "# Time parameters\n",
    "t0 = 0.0\n",
    "t_final = 5.0  # Adjust the final time as needed\n",
    "delta_t = 0.1  # Adjust the time step as needed\n",
    "\n",
    "# Number of time steps\n",
    "num_steps = int((t_final - t0) / delta_t)\n",
    "\n",
    "# Create tensors for time and solution\n",
    "t = torch.linspace(t0, t_final, num_steps + 1, requires_grad=True).reshape(-1, 1)\n",
    "y_true = y0 * torch.exp(0.5 * t)  # True solution for comparison\n",
    "\n",
    "# Instantiate the neural network, loss function, and optimizer\n",
    "model = TwoLayerNN()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training the neural network to approximate the solution\n",
    "for epoch in range(1000):  # You can adjust the number of epochs as needed\n",
    "    # Predict y values using the neural network\n",
    "    y_pred = model(t)\n",
    "    \n",
    "    # Compute the custom loss function\n",
    "    loss = custom_loss(y_pred, y_true, t)\n",
    "    \n",
    "    # Perform backpropagation and optimization step\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print the loss at every 100 epochs\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item():.6f}')\n",
    "\n",
    "# Evaluate the trained model at t_final\n",
    "predicted_y = model(torch.tensor([[t_final]], requires_grad=True))\n",
    "print(f'Predicted y({t_final}) = {predicted_y.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944c20ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
